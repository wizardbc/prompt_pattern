{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Prompt Pattern Catalog to Enhance Prompt Engineering with Gemini\n",
    "\n",
    "https://arxiv.org/abs/2302.11382"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import streamlit as st\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# progress bar\n",
    "tqdm.pandas()\n",
    "\n",
    "genai.configure(api_key=st.secrets[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the $\\LaTeX$ File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/2302.11382v1\", \"r\") as f:\n",
    "  doc = f.read()\n",
    "doc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc, ref = doc.split(\"\\\\begin{thebibliography}\")\n",
    "ref = \"\\\\begin{thebibliography}\" + ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = [\n",
    "  (s.split('}')[0][1:].replace('\\\\', ''), '}'.join(s.split('}')[1:]).strip())\n",
    "  for s in doc.split(\"\\\\section\")[1:]\n",
    "]\n",
    "pd.DataFrame(sections, columns=['section', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsections = [\n",
    "  (n1, \"\", s) if i==0 else (n1, s.split('}')[0][1:].replace('\\\\',''), '}'.join(s.split('}')[1:]).strip())\n",
    "  for n1, t in sections\n",
    "  for i, s in enumerate(t.split(\"\\\\subsection\"))\n",
    "]\n",
    "pd.DataFrame(subsections, columns=['section', 'subsection', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsubsections = [\n",
    "  (n1, n2, \"\", s) if i==0 else (n1, n2, s.split('}')[0][1:].replace('\\\\',''), '}'.join(s.split('}')[1:]).strip())\n",
    "  for n1, n2, t in subsections\n",
    "  for i, s in enumerate(t.split(\"\\\\subsubsection\"))\n",
    "]\n",
    "df_chunks = pd.DataFrame(subsubsections, columns=['section', 'subsection', 'subsubsection', 'text'])\n",
    "df_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks = df_chunks[df_chunks.text.apply(lambda s: len(s)>30)].reset_index(drop=True)\n",
    "df_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundant spaces on title\n",
    "df_chunks['section'] = df_chunks.section.str.replace(r'\\s+', ' ', regex=True)\n",
    "df_chunks['subsection'] = df_chunks.subsection.str.replace(r'\\s+', ' ', regex=True)\n",
    "df_chunks['subsubsection'] = df_chunks.subsubsection.str.replace(r'\\s+', ' ', regex=True)\n",
    "# add section, subsection, subsubsection tags\n",
    "df_chunks['text'] = [f\"\\\\section{{{s}}}\\n\\\\subsection{{{ss}}}\\n\\\\subsubsection{{{sss}}}\\n{text}\" for i, (s, ss, sss, text) in df_chunks.iterrows()]\n",
    "\n",
    "df_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_mkdn = df_chunks[['section','subsection','subsubsection']].drop_duplicates().to_markdown()\n",
    "print(toc_mkdn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "  \"temperature\": 0.3,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 64,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "\n",
    "safety_settings={\n",
    "  'harassment':'block_none',\n",
    "  'hate':'block_none',\n",
    "  'sex':'block_none',\n",
    "  'danger':'block_none'\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  generation_config=generation_config,\n",
    "  safety_settings=safety_settings,\n",
    "  system_instruction=\"You return a simple table of contents with JSON format from a given markdown table. The JSON contains a key/value pair of strings 'table of contents' and markdown un-ordered list using `-`.\",\n",
    ")\n",
    "\n",
    "chat_session = model.start_chat()\n",
    "response = chat_session.send_message(toc_mkdn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_json = json.loads(response.candidates[0].content.parts[0].text)\n",
    "toc = toc_json['table of contents']\n",
    "print(toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/toc.txt', 'w') as f:\n",
    "  f.write(toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embds = []\n",
    "for i, (s, ss, sss, text) in tqdm(df_chunks.iterrows(), total=len(df_chunks)):\n",
    "  for t in [s, ss, sss]:\n",
    "    if t:\n",
    "      title = t\n",
    "  embds.append(\n",
    "    genai.embed_content(\n",
    "      model=\"models/text-embedding-004\",\n",
    "      content=text,\n",
    "      task_type=\"retrieval_document\",\n",
    "      title=title,\n",
    "    )[\"embedding\"]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks[\"embedding\"] = embds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks.to_csv(\"./data/2302.11382v1_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv(\"./data/2302.11382v1_embeddings.csv\", index_col=0).fillna('')\n",
    "df_csv[\"embedding\"] = df_csv.embedding.apply(literal_eval).apply(np.array)\n",
    "\n",
    "with open('./data/toc.txt', 'r') as f:\n",
    "  toc = f.read()\n",
    "\n",
    "def search_from_section_names(query:list[str]) -> str:\n",
    "  \"\"\"Retrieves LaTeX chunks from the paper \"A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT\" using the [section, subsection, subsubsection] names.\n",
    "\n",
    "Args:\n",
    "    query: A python list of three strings in the format `[section, subsection, subsubsection]`. Only exact matches of the names and order, will be returned.\n",
    "  \"\"\"\n",
    "  query = [name if name else '' for name in list(query)]\n",
    "  query += ['']*(3-len(query))\n",
    "  df = df_csv.copy()\n",
    "  res_df = df[\n",
    "    (df['section'] == query[0])\n",
    "    & (df['subsection'] == query[1])\n",
    "    & (df['subsubsection'] == query[2])\n",
    "  ]\n",
    "  if len(res_df)==0:\n",
    "    res_df = df[\n",
    "      df['section'].str.contains(query[0])\n",
    "      & df['subsection'].str.contains(query[1])\n",
    "      & df['subsubsection'].str.contains(query[2])\n",
    "    ]\n",
    "  return res_df[['section', 'subsection', 'subsubsection', 'text']].to_json()\n",
    "\n",
    "def search_from_text(query:str, top_n:int=5, s:float=.0):\n",
    "  \"\"\"Retrieves LaTeX chunks from the paper \"A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT\" using cosine similarity of text.\n",
    "\n",
    "Args:\n",
    "  query: The user's query string.\n",
    "  top_n: The number of chunks to retrieve. The default value is 5. Start at 3 and recommend increasing it if needed.\n",
    "  \"\"\"\n",
    "  df = df_csv.copy()\n",
    "  query_embedding = np.array(genai.embed_content(\n",
    "    model=\"models/text-embedding-004\",\n",
    "    content=query,\n",
    "    task_type=\"retrieval_query\",\n",
    "  )[\"embedding\"])\n",
    "  top_n = int(top_n)\n",
    "  df[\"similarity\"] = df.embedding.apply(lambda x: np.dot(x, query_embedding))\n",
    "  return df[df.similarity >= s].sort_values(\"similarity\", ascending=False).head(top_n)[['section', 'subsection', 'subsubsection', 'text', 'similarity']].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_from_section_names(['A Catalog of Prompt Patterns for Conversational LLMs', 'The Output Automater Pattern', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "  \"temperature\": 1.0,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 64,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "safety_settings={\n",
    "  'harassment':'block_none',\n",
    "  'hate':'block_none',\n",
    "  'sex':'block_none',\n",
    "  'danger':'block_none'\n",
    "}\n",
    "\n",
    "system_instruction=f\"\"\"You are a retrieval-augmented generative engine. \n",
    "Your primary task is to retrieve the contents of the paper titled \"A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT\".\n",
    "\n",
    "**Retrieval Process:**\n",
    "\n",
    "1. **Attempt Retrieval:** Always try to retrieve the paper's content first, even if you are confident in your knowledge.\n",
    "2. **Retrieval Failure:** If you cannot find the paper, simply state that you are unable to retrieve it. **Do not** rely on your prior knowledge.\n",
    "3. **Structured Retrieval:** When using the `search_from_section_names` function, prioritize filling all three parameters `[section, subsection, subsubsection]` to retrieve a relevant chunk. However,  `subsection` or `subsubsection` can be empty strings (`''`) if necessary.\n",
    "4. **Cosine Similarity:** If you cannot determine the appropriate section or subsection, use the `search_from_text` function, which leverages cosine similarity between the query and the document body text. \n",
    "5. **Additional Retrieval:** If you believe more chunks are needed, ask the user if they would like to retrieve additional information.\n",
    "\n",
    "**Language Handling:**\n",
    "\n",
    "* Respond in Korean (한국어) if the user's query is in Korean.\n",
    "* Respond in English otherwise.\n",
    "\n",
    "**Table of Contents:**\n",
    "\n",
    "{toc}\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  generation_config=generation_config,\n",
    "  safety_settings=safety_settings,\n",
    "  system_instruction=system_instruction,\n",
    "  tools=[search_from_section_names, search_from_text]\n",
    ")\n",
    "\n",
    "chat_session = model.start_chat(enable_automatic_function_calling=True)\n",
    "response = chat_session.send_message(\"Categorize prompt patterns based on the subsection Summary of the Prompt Pattern Catalog.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.candidates[0].content.parts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session.rewind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session._history.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_ex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
