import numpy as np
import pandas as pd
from io import StringIO
from ast import literal_eval
import google.generativeai as genai
import streamlit as st

st.set_page_config(
    page_title="Chat with Paper",
    page_icon=":books:",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get help': 'https://github.com/wizardbc/prompt_pattern',
        'Report a bug': "https://github.com/wizardbc/prompt_pattern/issues",
        'About': "# Chat with Paper\nMade by Byung Chun Kim\n\nhttps://github.com/wizardbc/prompt_pattern"
    }
)

# load data
df_csv = pd.read_csv("./data/2302.11382v1_embeddings.csv", index_col=0).fillna('')
df_csv["embedding"] = df_csv.embedding.apply(literal_eval).apply(np.array)

with open('./data/toc.txt', 'r') as f:
  toc = f.read()

# search tools
def search_from_section_names(query:list[str]) -> str:
  """Retrieves LaTeX chunks from the paper "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT" using the [section, subsection, subsubsection] names.

Args:
    query: A python list of three strings in the format `[section, subsection, subsubsection]`. Only exact matches of the names and order, will be returned.
  """
  query = [name if name else '' for name in list(query)]
  query += ['']*(3-len(query))
  df = df_csv.copy()
  return df[
    (df['section'] == query[0])
    & (df['subsection'] == query[1])
    & (df['subsubsection'] == query[2])
  ][['section', 'subsection', 'subsubsection', 'text']].to_json()

def search_from_text(query:str, top_n:int=5, s:float=.0):
  """Retrieves LaTeX chunks from the paper "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT" using cosine similarity of text.

Args:
  query: The user's query string.
  top_n: The number of chunks to retrieve. The default value is 5. Start at 3 and recommend increasing it if needed.
  """
  df = df_csv.copy()
  query_embedding = np.array(genai.embed_content(
    model="models/text-embedding-004",
    content=query,
    task_type="retrieval_query",
  )["embedding"])
  top_n = int(top_n)
  df["similarity"] = df.embedding.apply(lambda x: np.dot(x, query_embedding))
  return df[df.similarity >= s].sort_values("similarity", ascending=False).head(top_n)[['text', 'similarity']].to_json()

# stream wrapper
# def gemini_stream_text(response):
#   for chunk in response:
#     if parts:=chunk.parts:
#       if text:=parts[0].text:
#         yield text

# kwargs to markdown
def kwargs2mkdn(_indent:int=0, **kwargs):
  chunk = [' '*_indent + f"- `{k}`: {v}" for k, v in kwargs.items()]
  return '\n'.join(chunk)

col1, col2 = st.columns(2, vertical_alignment='bottom')

with col1:
  st.title("ğŸ’¬ Chat with Paper")
  st.caption(":books: \"A Prompt Pattern Catalog to Enhance Prompt Engineering\" with Gemini 1.5")
  st.write("https://arxiv.org/abs/2302.11382")
  with st.container(border=True):
    st.write("ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ê°€ì ¸ì™€ ëŒ€ë‹µí•´ ì¤ë‹ˆë‹¤.")
    st.write("ë‚´ìš©ì„ ê°€ì ¸ì˜¤ê¸°ìœ„í•´ ë‘ê°€ì§€ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n- `search_from_section_names`: ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì´ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ë…¼ë¬¸ì˜ ì„¹ì…˜ì´ë¦„ì„ ì¶”ì¶œí•˜ì—¬ í•´ë‹¹ ì„¹ì…˜ì˜ ë‚´ìš©ì„ ê°€ì ¸ ì˜µë‹ˆë‹¤.\n- `search_from_text`: ë…¼ë¬¸ì˜ ì„¹ì…˜ë‚´ìš©ì„ ë‚˜íƒ€ë‚´ëŠ” embeddingê³¼ ì§ˆë¬¸ì˜ embeddingì˜ cosine similarityë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚´ìš©ì„ ê°€ì ¸ ì˜µë‹ˆë‹¤.\n\nì–¸ì–´ëª¨ë¸ì´ ì–´ë–¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í• ì§€ íŒë‹¨í•˜ë©°, ëª…ì‹œì ìœ¼ë¡œ í•¨ìˆ˜ëª…ì„ ì–¸ê¸‰í•˜ì—¬ í•´ë‹¹ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì§€ì‹œ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\nì•„ë˜ì˜ ëª©ì°¨ë¥¼ ì°¸ê³ í•˜ì—¬ ë¬¸ì„œì™€ ëŒ€í™” í•´ ë³´ì„¸ìš”.")
  st.markdown("**Table of Contents:**")
  with st.expander("Introduction"):
    st.write("ë³¸ ë…¼ë¬¸ì€ ëŒ€í™”í˜• ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‘ìš© ë¶„ì•¼ë¥¼ í™•ì¥í•˜ê¸° ìœ„í•´ í”„ë¡¬í”„íŠ¸ íŒ¨í„´ì„ ë„ì…í•©ë‹ˆë‹¤.")
  with st.expander("Comparing Software Patterns with Prompt Patterns"):
    st.write("ë³¸ ì„¹ì…˜ì—ì„œëŠ” ì†Œí”„íŠ¸ì›¨ì–´ íŒ¨í„´ê³¼ í”„ë¡¬í”„íŠ¸ íŒ¨í„´ì„ ë¹„êµí•˜ì—¬ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.\n\nSub-sections:\n- Overview of Software Patterns\n- Overview of Prompt Patterns\n- Evaluating Means for Defining a Prompt Pattern's Structure and Ideas\n- A Way Forward: Fundamental Contextual Statements")
  with st.expander("A Catalog of Prompt Patterns for Conversational LLMs"):
    st.write("ë³¸ ì„¹ì…˜ì—ì„œëŠ” ëŒ€í™”í˜• LLM ìƒí˜¸ì‘ìš© ë° ì¶œë ¥ ìƒì„±ì„ ìœ„í•œ ì†Œí”„íŠ¸ì›¨ì–´ ì‘ì—… ìë™í™”ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ íŒ¨í„´ ì¹´íƒˆë¡œê·¸ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.\n\nSub-sections:\n- Summary of the Prompt Pattern Catalog\n- The Meta Language Creation Pattern\n- The Output Automater Pattern\n- The Flipped Interaction Pattern\n- The Persona Pattern\n- The Question Refinement Pattern\n- The Alternative Approaches Pattern\n- The Cognitive Verifier Pattern\n- The Fact Check List Pattern\n- The Template Pattern\n- The Infinite Generation Pattern\n- The Visualization Generator Pattern\n- The Game Play Pattern\n- The Reflection Pattern\n- The Refusal Breaker Pattern\n- The Context Manager Pattern\n- The Recipe Pattern\n\nEach subsection, except the first, has subsubsections:\n- Intent and Context\n- Motivation\n- Structure and Key Ideas\n- Example Implementation\n- Consequences")
  with st.expander("Related Work"):
    st.write("ë³¸ ì„¹ì…˜ì—ì„œëŠ” ì†Œí”„íŠ¸ì›¨ì–´ íŒ¨í„´ê³¼ í”„ë¡¬í”„íŠ¸ ì„¤ê³„ì— ëŒ€í•œ ê¸°ì¡´ ì—°êµ¬ ë° LLM, íŠ¹íˆ ChatGPTì˜ ì„±ëŠ¥ í‰ê°€ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.")
  with st.expander("Concluding Remarks"):
    st.write("ë³¸ ë…¼ë¬¸ì€ ChatGPTì™€ ê°™ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ íŒ¨í„´ ì¹´íƒˆë¡œê·¸ë¥¼ ë¬¸ì„œí™”í•˜ê³  ì ìš©í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•˜ë©°, í”„ë¡¬í”„íŠ¸ íŒ¨í„´ ì„¤ê³„ë¥¼ ê°œì„ í•˜ì—¬ ëŒ€í™”í˜• LLMì„ ìœ„í•œ ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì°½ì¶œí•˜ëŠ” ì—°êµ¬ë¥¼ ì¥ë ¤í•˜ê³ ì í•©ë‹ˆë‹¤.")

# Google API key
if "api_key" not in st.session_state:
  try:
    st.session_state.api_key = st.secrets["GOOGLE_API_KEY"]
  except:
    st.session_state.api_key = ''
    st.warning("Your Google API Key is not provided in `.streamlit/secrets.toml`, but you can input one in the sidebar for temporary use.")

# Sidebar for parameters
with st.sidebar:
  # Google API Key
  if st.session_state.api_key:
    genai.configure(api_key=st.session_state.api_key)
  else:
    st.session_state.api_key = st.text_input("Google API Key", type="password")

  st.subheader("Visible")
  system_checkbox = st.checkbox("system", value=False)
  f_call_checkbox = st.checkbox("tool", value=False)

  # ChatCompletion parameters
  st.header("Gemini Parameters")
  model_name = st.selectbox("model", ["gemini-1.5-flash", "gemini-1.5-pro"])
  generation_config = {
    "temperature": st.slider("temperature", min_value=0.0, max_value=1.0, value=1.0),
    "top_p": st.slider("top_p", min_value=0.0, max_value=1.0, value=0.95),
    "top_k": st.number_input("top_k", min_value=1, value=64),
    "max_output_tokens": st.number_input("max_output_tokens", min_value=1, value=8192),
  }
  
  # retriever parameters
  # st.header("Retriever Parameters")
  # n_chunks = st.number_input("number of chunks", min_value=1, value=5)

safety_settings={
  'harassment':'block_none',
  'hate':'block_none',
  'sex':'block_none',
  'danger':'block_none'
}

system_instruction=f"""You are an experienced prompt engineer.
You can retrieve the contents of the paper titled 'A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT'.

If you are not sure, then just say you don't know; never make up a story.
When you use the function `search_from_section_names`, first, you try fill all the three `[section, subsection, subsubsection]` names to get one or two chunks.
If you think we need more chunks, then ask the user want to get more.
If you cannot determine which section or (sub)subsection should be chosen, use the function `search_from_text` which is using cosine similarity of the query and the document body text.

You have to use Korean (í•œêµ­ì–´) if the user asks in Korean (í•œêµ­ì–´).
Otherwise you must use English.

Table of Contents (each depth means [section, subsection, subsubsection]):\n{toc}"""

if "chat_session" in st.session_state:
  chat_session = st.session_state.chat_session
else:
  model = genai.GenerativeModel(
    model_name=model_name,
    generation_config=generation_config,
    safety_settings=safety_settings,
    system_instruction=system_instruction,
    tools=[search_from_section_names, search_from_text],
  )
  chat_session = model.start_chat(enable_automatic_function_calling=True)
  st.session_state.chat_session = chat_session
  # with st.spinner("Generating..."):
  #   response = chat_session.send_message("Give me a simplified ToC containing all the section and subsection names, not subsubsection.")
  # st.rerun()

with st.sidebar:
  # chat controls
  st.header("Chat Control")
  # chat_role = st.selectbox("role", ["system", "model", "user", "tool"], index=1)
  btn_col1, btn_col2 = st.columns(2)
  with btn_col1:
    st.button("Rewind", on_click=chat_session.rewind, use_container_width=True, type='primary')
  with btn_col2:
    st.button("Clear", on_click=chat_session._history.clear, use_container_width=True)

with col2:
  messages = st.container()
  # display system instruction
  if system_checkbox:
    with messages.chat_message("system"):
      st.write(system_instruction)

  # display messages in history
  for content in st.session_state.chat_session.history:
    for part in content.parts:
      if text:=part.text:
        with messages.chat_message('human' if content.role == 'user' else 'ai'):
          st.write(text)
      if f_call_checkbox:
        if fc:=part.function_call:
          with messages.chat_message('ai'):
            st.write(f"Function Call\n- name: {fc.name}\n- args\n{kwargs2mkdn(4, **fc.args)}")
        if fr:=part.function_response:
          with messages.chat_message('human'):
            st.write(f"Function Response\n- name: {fr.name}\n- response\n  - `result`")
            st.json(fr.response["result"])
      else:
        if fr:=part.function_response:
          with messages.chat_message('ai'):
            st.dataframe(pd.read_json(StringIO(fr.response["result"]))[['section', 'subsection', 'subsubsection']])

  # chat input
  if prompt := st.chat_input("Ask me anything...", disabled=False if st.session_state.api_key else True):
    with messages.chat_message('human'):
      st.write(prompt)
    with messages.chat_message('ai'):
      with st.spinner("Generating..."):
        # response = chat_session.send_message(prompt, stream=True)
        response = chat_session.send_message(prompt)
      # st.write_stream(gemini_stream_text(response))
    st.rerun()